{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bluehouseblah.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaQ3FT3vcdkJ/Nz9OcWkTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dunnohow/Bert/blob/master/bluehouseblah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPvW1KQDCFtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "a98f325a-6c87-45a1-d1a6-ce81d1d84d64"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CWo8QmCCW9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, BertTokenizer # load language model & tokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import random\n",
        "import tqdm\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQFGst_KwSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b914fe73-6f79-44da-c86b-0cc3516e1c54"
      },
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMwQGySbDOaj",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "#**Load Data (청와대 청원 데이터)**\n",
        "\n",
        "<br>\n",
        "\n",
        "https://dacon.io/competitions/open/235597/overview/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1suMzANTDKgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e7c3197e-5c25-4d1a-fdf7-f70ab5771f45"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sECt9SIoDivG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8a1a2f19-475f-474e-a4c7-1c2a27eba510"
      },
      "source": [
        "path = \"drive/My Drive/NLP_practice/Blue-House/\"\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv',\n",
              " 'sample_submission.csv',\n",
              " 'test.csv',\n",
              " 'data.csv',\n",
              " 'bluehouseblah.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ7O2sK1D7_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그래프에 retina display 적용\n",
        "train = pd.read_csv(path + 'train.csv')\n",
        "test = pd.read_csv(path + 'test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I3tJ4-uELoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a633e64f-1c91-43ac-d91d-c049031c301e"
      },
      "source": [
        "train.iloc[:,1].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    13362\n",
              "1    13337\n",
              "0    13301\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRxrqWMTpYF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8dda6b6e-5cae-454f-869b-d010b751466d"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>category</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>신혼부부위한 주택정책 보다 보육시설 늘려주세요.. 국민세금으로 일부를 위한 정책펴지...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>학교이름에 '남자'도 붙여주세요. 울산여자중학교에 재학중인 학생입니다 최근 양성평등...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>빙상연맹, 대한축구협회등 각종 체육협회의 비리를 철저하게 밝혀주세요.. 최근 동계올...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>티비 12세,15세 관람가도 연령확인 의무화 하자.. 제기 에전에 티비를 보다가 잠...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>무더운 여름철엔 남성들도 시원한 자율복장을 해야. 무더운 여름철에는 남성들도 노넥타...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  category                                               data\n",
              "0      0         2  신혼부부위한 주택정책 보다 보육시설 늘려주세요.. 국민세금으로 일부를 위한 정책펴지...\n",
              "1      1         0  학교이름에 '남자'도 붙여주세요. 울산여자중학교에 재학중인 학생입니다 최근 양성평등...\n",
              "2      2         1  빙상연맹, 대한축구협회등 각종 체육협회의 비리를 철저하게 밝혀주세요.. 최근 동계올...\n",
              "3      3         1  티비 12세,15세 관람가도 연령확인 의무화 하자.. 제기 에전에 티비를 보다가 잠...\n",
              "4      4         1  무더운 여름철엔 남성들도 시원한 자율복장을 해야. 무더운 여름철에는 남성들도 노넥타..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLtVvTnHzHce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c2afee7a-75b6-4205-d3f3-324803d9f0ef"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>소년법 폐지해주세요. 법 아래에서 보호받아야 할 아이들이\\n법으로 인해 보호받지 못...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>국공립 유치원 증설에 관하여. 국공립 유치원 부지 학보와건립및 증설에\\n*지역 어린...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>나경원파면. 나경원의원의  동계올림픽 위원을 파면해 주세요</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>국민위원에가 삼성편만들어요. 삼성에서 11년간  일하고 혈암과 백혈병 진단을 받은 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>방과후,유치원,어린이집 영어교육을 유지시켜주세요. 저는 아이 셋 키우는 평범한 주부...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                               data\n",
              "0      0  소년법 폐지해주세요. 법 아래에서 보호받아야 할 아이들이\\n법으로 인해 보호받지 못...\n",
              "1      1  국공립 유치원 증설에 관하여. 국공립 유치원 부지 학보와건립및 증설에\\n*지역 어린...\n",
              "2      2                   나경원파면. 나경원의원의  동계올림픽 위원을 파면해 주세요\n",
              "3      3  국민위원에가 삼성편만들어요. 삼성에서 11년간  일하고 혈암과 백혈병 진단을 받은 ...\n",
              "4      4  방과후,유치원,어린이집 영어교육을 유지시켜주세요. 저는 아이 셋 키우는 평범한 주부..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Ip8B3QGzOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-multilingual-cased').cuda()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpKwJbvRFNc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7f4535a5-5272-45a3-c806-7452b8a201d5"
      },
      "source": [
        "# 데이터 문장 길이가 다르기 때문에 [PAD] token을 활용해야함, 또한 최대 길이도 정해야함\n",
        "print(train['data'][0])\n",
        "print(tokenizer.encode(train['data'][0]))\n",
        "print(len(tokenizer.encode(train['data'][0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "신혼부부위한 주택정책 보다 보육시설 늘려주세요.. 국민세금으로 일부를 위한 정책펴지 마시고\\n보편적으로 모든국민이 수긍할  수 있는 복지정책 펴 주시길 바랍니다.\\n저도 신혼부부이지만 당첨되는 사람 로또되는 이런주택정책 반대합니다.\\n국민세금을 일부 사람들에게 퍼주기식이 되면 안되죠..\\n그 세금으로 우리아이 안전하게 맡길 수 있는 보육시설을 전국에 설치해 주세요..\\n대기업들은 솔선수범해서 모든 사업장에 의무설치 할 수 있도록 하시구요..\\n집 보다 애 맡길데가 없어 경력단절 되는게 더 괴롭습니다.!\\n집은 개인의 능력을 키워 사는게 맞습니다.\\n그 능력을 키울수 있도록 육아 전담에 힘을 기울이는게 맞습니다.\\n우리아이 부모가 키우는거 맞지만 이제는 국가가\\n책임지는 시대로 가는게 맞다고 봅니다.\\n그렇잖아도 부동산 가격 자꾸 올라가는게 정부정책이 잘못 되었다고 봅니다.\\n부동산은 그냥 내버려 두세요!  좀!\\n건들수록 역효과네요..\n",
            "[101, 9487, 119439, 14646, 14646, 19855, 11102, 9689, 119342, 16605, 119254, 106154, 9356, 83811, 14040, 31928, 9044, 26737, 16323, 24982, 48549, 119, 119, 8909, 36553, 24982, 40032, 11467, 47807, 11513, 28195, 9670, 119254, 119396, 12508, 9246, 14040, 11664, 165, 182, 30005, 50450, 17022, 25701, 20479, 36553, 10739, 9460, 118665, 14843, 9460, 13767, 9357, 12508, 16605, 119254, 9923, 9689, 14040, 118666, 9318, 118853, 48345, 119, 165, 182, 48387, 12092, 9487, 119439, 14646, 14646, 44359, 19105, 9067, 119260, 24683, 9405, 61250, 9202, 118839, 24683, 80956, 16323, 119342, 16605, 119254, 9321, 14423, 33188, 48345, 119, 165, 182, 20479, 36553, 24982, 40032, 10622, 47807, 9405, 61250, 61688, 9913, 16323, 12310, 21155, 10739, 9098, 14867, 9521, 118800, 119217, 119, 119, 165, 182, 78136, 9435, 40032, 11467, 9604, 31065, 10739, 9521, 16617, 17594, 9257, 118666, 9460, 13767, 9356, 83811, 14040, 31928, 10622, 9665, 20479, 10530, 102221, 14523, 9689, 24982, 48549, 119, 119, 165, 182, 14423, 12310, 26784, 22879, 9451, 18471, 15891, 108056, 70146, 25701, 9405, 26784, 85903, 9637, 32537, 31928, 18622, 9955, 9460, 107931, 9952, 14040, 17196, 48549, 119, 119, 165, 182, 38696, 106154, 9532, 9257, 118666, 28911, 11287, 9555, 12965, 8885, 28143, 24989, 58931, 54780, 14153, 9074, 8905, 118884, 119081, 48345, 119, 106, 165, 182, 38696, 10892, 8857, 48418, 9046, 33975, 9838, 69592, 9405, 11018, 14153, 9256, 119081, 48345, 119, 165, 182, 78136, 9046, 33975, 9838, 78123, 15891, 107931, 9626, 16985, 9665, 105462, 10530, 10028, 10622, 8932, 78123, 31728, 14153, 9256, 119081, 48345, 119, 165, 182, 27355, 31065, 10739, 9365, 39420, 11287, 9838, 27355, 11018, 41521, 9256, 28578, 9638, 87164, 93222, 11287, 165, 182, 119254, 36240, 32815, 102080, 11261, 8843, 11018, 14153, 9256, 85634, 100, 119, 165, 100, 9365, 18778, 21386, 8843, 45465, 9651, 118694, 9583, 17342, 68828, 14153, 9670, 14646, 16605, 119254, 10739, 9654, 118940, 17737, 11664, 100, 119, 165, 182, 14646, 18778, 21386, 10892, 8924, 118729, 8996, 41605, 26737, 9102, 24982, 48549, 106, 9682, 106, 165, 182, 71439, 27023, 15891, 31398, 9566, 119449, 11882, 77884, 48549, 119, 119, 102]\n",
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScQQeSEyPnW",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "#**훈련셋 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcxZpFOfrb4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc4b8714-fb64-4a5e-fa94-9b22e45db8ea"
      },
      "source": [
        "# 청원 문장 추출\n",
        "sentences = train['data']\n",
        "def sentence_clean(text):\n",
        "    # 스페셜 토큰 추가\n",
        "    text = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in text]\n",
        "    # 문장 단위\"\\\\n\" 제거\n",
        "    text = [sentence.replace('\\\\n', '.') for sentence in text]\n",
        "    text = [sentence.replace('.', ' ') for sentence in text]\n",
        "    text = [sentence.replace('!', ' ') for sentence in text]\n",
        "    text = [sentence.replace('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ') for sentence in text]\n",
        "    text = [tokenizer.tokenize(sent) for sent in text]\n",
        "    return(text)\n",
        "\n",
        "tokenized_texts = sentence_clean(sentences)\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '신', '##혼', '##부', '##부', '##위', '##한', '주', '##택', '##정', '##책', '보다', '보', '##육', '##시', '##설', '늘', '##려', '##주', '##세', '##요', '국', '##민', '##세', '##금', '##으로', '일부', '##를', '위한', '정', '##책', '##펴', '##지', '마', '##시', '##고', '보', '##편', '##적으로', '모든', '##국', '##민', '##이', '수', '##긍', '##할', '수', '있는', '복', '##지', '##정', '##책', '펴', '주', '##시', '##길', '바', '##랍', '##니다', '저', '##도', '신', '##혼', '##부', '##부', '##이지', '##만', '당', '##첨', '##되는', '사', '##람', '로', '##또', '##되는', '이런', '##주', '##택', '##정', '##책', '반', '##대', '##합', '##니다', '국', '##민', '##세', '##금', '##을', '일부', '사', '##람', '##들에게', '퍼', '##주', '##기', '##식', '##이', '되', '##면', '안', '##되', '##죠', '그', '세', '##금', '##으로', '우', '##리아', '##이', '안', '##전', '##하게', '맡', '##길', '수', '있는', '보', '##육', '##시', '##설', '##을', '전', '##국', '##에', '설치', '##해', '주', '##세', '##요', '대', '##기', '##업', '##들은', '솔', '##선', '##수', '##범', '##해서', '모든', '사', '##업', '##장에', '의', '##무', '##설', '##치', '할', '수', '있도록', '하', '##시', '##구', '##요', '집', '보다', '애', '맡', '##길', '##데', '##가', '없', '##어', '경', '##력', '##단', '##절', '되는', '##게', '더', '괴', '##롭', '##습', '##니다', '집', '##은', '개', '##인의', '능', '##력을', '키', '##워', '사', '##는', '##게', '맞', '##습', '##니다', '그', '능', '##력을', '키', '##울', '##수', '있도록', '육', '##아', '전', '##담', '##에', '힘', '##을', '기', '##울', '##이는', '##게', '맞', '##습', '##니다', '우', '##리아', '##이', '부', '##모', '##가', '키', '##우', '##는', '##거', '맞', '##지만', '이', '##제는', '국가', '##가', '책', '##임', '##지는', '시대', '##로', '가', '##는', '##게', '맞', '##다고', '[UNK]', '[UNK]', '부', '##동', '##산', '가', '##격', '자', '##꾸', '올', '##라', '##가는', '##게', '정', '##부', '##정', '##책', '##이', '잘', '##못', '되었다', '##고', '[UNK]', '부', '##동', '##산', '##은', '그', '##냥', '내', '##버', '##려', '두', '##세', '##요', '좀', '건', '##들', '##수', '##록', '역', '##효', '##과', '##네', '##요', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygw3MKLozBWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55562817-78c4-48a9-fd0d-d6f9b444d05d"
      },
      "source": [
        "token_len = np.array([len(sent) for sent in tokenized_texts])\n",
        "np.quantile([len(sent) for sent in tokenized_texts],0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "719.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik-WKMPSXqfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "1abe567f-ad37-4b91-eb0a-ed34b5c7f480"
      },
      "source": [
        "plt.hist([len(sent) for sent in tokenized_texts])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.9422e+04, 4.6200e+02, 7.9000e+01, 2.3000e+01, 8.0000e+00,\n",
              "        3.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
              " array([2.00000e+00, 2.07150e+03, 4.14100e+03, 6.21050e+03, 8.28000e+03,\n",
              "        1.03495e+04, 1.24190e+04, 1.44885e+04, 1.65580e+04, 1.86275e+04,\n",
              "        2.06970e+04]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHtIozi45EVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "90150b22-1feb-4082-bb6c-74fcafaf7ec0"
      },
      "source": [
        "trim = train.iloc[np.where(token_len < 719)[0],:]\n",
        "trim.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>category</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>신혼부부위한 주택정책 보다 보육시설 늘려주세요.. 국민세금으로 일부를 위한 정책펴지...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>학교이름에 '남자'도 붙여주세요. 울산여자중학교에 재학중인 학생입니다 최근 양성평등...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>빙상연맹, 대한축구협회등 각종 체육협회의 비리를 철저하게 밝혀주세요.. 최근 동계올...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>티비 12세,15세 관람가도 연령확인 의무화 하자.. 제기 에전에 티비를 보다가 잠...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>무더운 여름철엔 남성들도 시원한 자율복장을 해야. 무더운 여름철에는 남성들도 노넥타...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  category                                               data\n",
              "0      0         2  신혼부부위한 주택정책 보다 보육시설 늘려주세요.. 국민세금으로 일부를 위한 정책펴지...\n",
              "1      1         0  학교이름에 '남자'도 붙여주세요. 울산여자중학교에 재학중인 학생입니다 최근 양성평등...\n",
              "2      2         1  빙상연맹, 대한축구협회등 각종 체육협회의 비리를 철저하게 밝혀주세요.. 최근 동계올...\n",
              "3      3         1  티비 12세,15세 관람가도 연령확인 의무화 하자.. 제기 에전에 티비를 보다가 잠...\n",
              "4      4         1  무더운 여름철엔 남성들도 시원한 자율복장을 해야. 무더운 여름철에는 남성들도 노넥타..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7F1qorGqym7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "866c3436-4467-4ed0-fb49-758c7ad4fd76"
      },
      "source": [
        "labels = trim['category']\n",
        "labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    12410\n",
              "2    11798\n",
              "0    11791\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRm5y85l9KO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cddfa5b1-c3c1-48c9-c470-e6a2be330430"
      },
      "source": [
        "sentences = trim['data']\n",
        "tokenized_texts = sentence_clean(sentences)\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '신', '##혼', '##부', '##부', '##위', '##한', '주', '##택', '##정', '##책', '보다', '보', '##육', '##시', '##설', '늘', '##려', '##주', '##세', '##요', '국', '##민', '##세', '##금', '##으로', '일부', '##를', '위한', '정', '##책', '##펴', '##지', '마', '##시', '##고', '보', '##편', '##적으로', '모든', '##국', '##민', '##이', '수', '##긍', '##할', '수', '있는', '복', '##지', '##정', '##책', '펴', '주', '##시', '##길', '바', '##랍', '##니다', '저', '##도', '신', '##혼', '##부', '##부', '##이지', '##만', '당', '##첨', '##되는', '사', '##람', '로', '##또', '##되는', '이런', '##주', '##택', '##정', '##책', '반', '##대', '##합', '##니다', '국', '##민', '##세', '##금', '##을', '일부', '사', '##람', '##들에게', '퍼', '##주', '##기', '##식', '##이', '되', '##면', '안', '##되', '##죠', '그', '세', '##금', '##으로', '우', '##리아', '##이', '안', '##전', '##하게', '맡', '##길', '수', '있는', '보', '##육', '##시', '##설', '##을', '전', '##국', '##에', '설치', '##해', '주', '##세', '##요', '대', '##기', '##업', '##들은', '솔', '##선', '##수', '##범', '##해서', '모든', '사', '##업', '##장에', '의', '##무', '##설', '##치', '할', '수', '있도록', '하', '##시', '##구', '##요', '집', '보다', '애', '맡', '##길', '##데', '##가', '없', '##어', '경', '##력', '##단', '##절', '되는', '##게', '더', '괴', '##롭', '##습', '##니다', '집', '##은', '개', '##인의', '능', '##력을', '키', '##워', '사', '##는', '##게', '맞', '##습', '##니다', '그', '능', '##력을', '키', '##울', '##수', '있도록', '육', '##아', '전', '##담', '##에', '힘', '##을', '기', '##울', '##이는', '##게', '맞', '##습', '##니다', '우', '##리아', '##이', '부', '##모', '##가', '키', '##우', '##는', '##거', '맞', '##지만', '이', '##제는', '국가', '##가', '책', '##임', '##지는', '시대', '##로', '가', '##는', '##게', '맞', '##다고', '[UNK]', '[UNK]', '부', '##동', '##산', '가', '##격', '자', '##꾸', '올', '##라', '##가는', '##게', '정', '##부', '##정', '##책', '##이', '잘', '##못', '되었다', '##고', '[UNK]', '부', '##동', '##산', '##은', '그', '##냥', '내', '##버', '##려', '두', '##세', '##요', '좀', '건', '##들', '##수', '##록', '역', '##효', '##과', '##네', '##요', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_LlS3oF-sfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "6a852f36-d33e-4367-a1b4-318a9de0c1de"
      },
      "source": [
        "plt.hist([len(sent) for sent in tokenized_texts])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7691., 7836., 6048., 4590., 3149., 2240., 1570., 1232.,  926.,\n",
              "         717.]),\n",
              " array([  2. ,  73.6, 145.2, 216.8, 288.4, 360. , 431.6, 503.2, 574.8,\n",
              "        646.4, 718. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd8r0dBfP-U1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "69326f95-4574-4833-8eeb-c1c2cd4b5b9c"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(len(tokenizer.convert_tokens_to_ids(tokenized_texts[0])))\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   9487, 119439,  14646,  14646,  19855,  11102,   9689,\n",
              "       119342,  16605, 119254, 106154,   9356,  83811,  14040,  31928,\n",
              "         9044,  26737,  16323,  24982,  48549,   8909,  36553,  24982,\n",
              "        40032,  11467,  47807,  11513,  28195,   9670, 119254, 119396,\n",
              "        12508,   9246,  14040,  11664,   9356,  50450,  17022,  25701,\n",
              "        20479,  36553,  10739,   9460, 118665,  14843,   9460,  13767,\n",
              "         9357,  12508,  16605, 119254,   9923,   9689,  14040, 118666,\n",
              "         9318, 118853,  48345,   9663,  12092,   9487, 119439,  14646,\n",
              "        14646,  44359,  19105,   9067, 119260,  24683,   9405,  61250,\n",
              "         9202, 118839,  24683,  80956,  16323, 119342,  16605, 119254,\n",
              "         9321,  14423,  33188,  48345,   8909,  36553,  24982,  40032,\n",
              "        10622,  47807,   9405,  61250,  61688,   9913,  16323,  12310,\n",
              "        21155,  10739,   9098,  14867,   9521, 118800, 119217,   8924,\n",
              "         9435,  40032,  11467,   9604,  31065,  10739,   9521,  16617,\n",
              "        17594,   9257, 118666,   9460,  13767,   9356,  83811,  14040,\n",
              "        31928,  10622,   9665,  20479,  10530, 102221,  14523,   9689])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGqiGoEqv-hD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "17ce4c8f-4aef-4eb9-a797-90279ab1606d"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETWylaNryCUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "fbabbf5f-083d-4b8b-e3f9-e577c099150b"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                    labels, \n",
        "                                                                                    random_state=7, \n",
        "                                                                                    test_size=0.2)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                       input_ids,\n",
        "                                                       random_state=7, \n",
        "                                                       test_size=0.2)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels.values)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels.values)\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(train_labels[0])\n",
        "print(train_masks[0])\n",
        "print(validation_inputs[0])\n",
        "print(validation_labels[0])\n",
        "print(validation_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   9095,  17138, 119439,  10622,   9957,  33768,  18227,   9960,\n",
            "         16323, 119085,  14040,  28188,   9095,  17138, 119439,  10622,   9321,\n",
            "         14423,  12178,  30050,   8924,   9651,  29683,  11261,   9095,  17138,\n",
            "        119121,  13764,  27023,  10530,  18154,   9730,  61844,  58303,  48345,\n",
            "          9365,  14646,  92413,    117,   9487, 119439,   9365,  14646,  92413,\n",
            "          9032,  85836,   9460,  13767,   9672,  12092,  14801,   9984, 119342,\n",
            "         10622,   9095,  17138, 119121,  13764,  60362,  89184,   8932,  14863,\n",
            "         20626,  23466,   9330,  17730,  24683,  30050,   9730,  61844,  58303,\n",
            "         48345,    102,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([   101,   9604,  12692,  25258,   8965,  16439,  32537,  20173,   8965,\n",
            "         10622,   9925, 119284,  15891,  91773,   8907,  83811,  21789,  18471,\n",
            "          9960,  16323,  24982,  48549,   9521, 118741,  35506,  24982,  48549,\n",
            "          9978,   9757, 101322,  23321,  24017,  10884,  23321,  10954,  10622,\n",
            "          9104,   9953,  14646,  39420,  58303,  48345,   9405,  25242, 119168,\n",
            "         10622,  89093,  89523,  17342,  89093,  89523,  11664,   9405,  25242,\n",
            "         83811,   9521,  12178,   8907,  83811,   9978,  83380,   9248,  93200,\n",
            "          9689,  24982,  48549,   9953,  25242,  11489,  17655,  23969,   8907,\n",
            "         83811,  10739,   9318, 118700,  12965,  21711,   9706,  40032,   8982,\n",
            "         17342,  11489,   9612,  12178,   9736,  10459,  17138,   8853,  11102,\n",
            "          9640,  36210,  11287,   9248,  93200,  65096,   9420,  66540,  33188,\n",
            "         48345,  93222,  11287,   9531,  13890,  12424,  12424,   8857, 119432,\n",
            "          9960,  16323, 119049,  21711,  89093, 119112,  10622, 118671,  48549,\n",
            "           136, 102080,  11018,   9352,  18227,  14523,  68828,  28911,   8907,\n",
            "         83811,  10892])\n",
            "tensor(2)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCsPw_Wtx64t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 16\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdIzKVfi1Yca",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "#**Test set pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q49M7fW-1aFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "fea96712-f6ac-47c0-8277-717eee04e0c6"
      },
      "source": [
        "# 청원 문장 추출\n",
        "sentences = test['data']\n",
        "# 스페셜 토큰 추가\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "# 문장 단위\"\\\\n\" 제거\n",
        "sentences = [sentence.replace('\\\\n', '.') for sentence in sentences]\n",
        "sentences = [sentence.replace('.', ' ') for sentence in sentences]\n",
        "sentences = [sentence.replace('!', ' ') for sentence in sentences]\n",
        "sentences = [sentence.replace('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ') for sentence in sentences]\n",
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 소년법 폐지해주세요  법 아래에서 보호받아야 할 아이들이 법으로 인해 보호받지 못하고 있습니다 오히려 법을 악용하는 사례만 늘어나고 그 강도는 높아지고 있습니다 소년법폐지를 부탁드립니다 [SEP]',\n",
              " '[CLS] 국공립 유치원 증설에 관하여  국공립 유치원 부지 학보와건립및 증설에 *지역 어린이 놀이터 부지와 지역의 방대한 주민센터휴계부지 및 구청 시청 군청 청사공간부지 활용과 청년실업과 퇴직희망자가 재교육을 통해  유아 유치 업무에 종사하는 방법은 불가능한 건가요 [SEP]',\n",
              " '[CLS] 나경원파면  나경원의원의  동계올림픽 위원을 파면해 주세요 [SEP]',\n",
              " '[CLS] 국민위원에가 삼성편만들어요  삼성에서 11년간  일하고 혈암과 백혈병 진단을 받은 사람이 많아요~  그래서 산업 제외을 받기 위해서 환경 평가표을 받아야 합니다 ~  그래야 신청 할 수 있습니다 ~ 법원에서도 평가표을 공개하라고 판결이 나지만   국가 국민위원에가 공개을 하지 못하겠 하고 있어요~ 삼성이 환경평간표가 산업 기밀 이라고 다시 막았어요~ 단 하루만에 피해자들은 11년동안 고통을 받고 있는데           제발  국민위원에가  국민을 위해  일해주세요 그리고 국민위원에서 독단으로  처리한 분도 다시 감독 해주세요 [SEP]',\n",
              " '[CLS] 방과후,유치원,어린이집 영어교육을 유지시켜주세요  저는 아이 셋 키우는 평범한 주부입니다 학교 방과후나 어린이집에서 받는 영어교육은 과하지않은 도움되는 교육이라 좋은점을 많이 느끼고 있는데 이렇게 없앤다고 하니 막막한 생각이 듭니다 학원을 보내기쉽지않은 경제상태인데 많은 시간을 배정받은 것도 아닌데   저처럼 도움받는 분들이 더욱 많은것으로 알고있는데 상황도 모르고 높은분들은 그저 토론 후 없애버리고있습니다   영어를 쉽게 놀이처럼 받아들이며 배우는 작은 지역에서의 영어교육은 서울 수도권처럼 과하지않습니다   영어유치원 원어민 선생님과 공부하는것도 아니고 소규모지역 부모들은 이 작은 교육도 지금 받지 못하게 됐어요 서울 수도권 영어교육은 규제해야될 정도로 과열되었을지 모르나 지역은 그렇지않습니다 수업일수도 여기는 부족하다싶은데 어디를 기준으로 이것이 시행되는지 모르겠습니다 무조건 없애고 줄인다는건 답이 없습니다 부디 한번더 두번더 생각하셔서 영어교육이 유지될수있도록 해주십시오 [SEP]',\n",
              " '[CLS] 유은혜는 당장 사퇴하라     능력도 전문성도 없는 사람이 국회의윈 그리고 대통령의 친분으로 인해 사회ㆍ교육부총리라는 중요한 자리에 낙하산으로 내정된 것은 적폐가 아나고 무엇이냐  부총리 자리의 경중도 모르고, 총선 출마에 대한 확고한 인식도 없는 인간이 무슨 부총리냐? 정부와 내각은 총 사퇴하라  정부는 제발 정신 좀 차리고, 나라 살림, 경제에 제발  신경 좀 써라  남북관계는  이 정부 아니어도 언젠가는 좋아지고, 통일은 아직도 먼 얘기이며, 칼자루는 우리가 아닌 미국이 갖고 있지 않는가  정신들 좀 차리고  살라  국민들이 얼마나 힘든지 너희들은 아는가? 각성하라    [SEP]',\n",
              " '[CLS] 신태용 감독  노벨상 수상 청원합니다      한국축구가 가장어려운 시기에,  용기있기 감독을 맡아서 16강 진출보다 더 값진 독일전 승리의 신화를 이룬 신태용 국가대표 감독에게 감사의 표시로 노벨상 수상을 청원합니다  [SEP]',\n",
              " '[CLS] 사회복무요원 최저임금 보장  사회복무요원들은 의식주 보장이 아무것도 되질 않는데 왜 최저임금을 보장해주지 않는거죠? 일 쉽게 한다고 배부른소리라고 할게 아니라 지킬건 지켜야 한다고 생각합니다 [SEP]',\n",
              " '[CLS] 로또복권  의구심  로또복귄 운영에  대한 민초들의  의구심을 해소하기 위하여 추첨은 생방송으로  진행하는것이  지극히 타당하다고 봅니다  하루속히  녹화방송을  생방송으로 바꿔주실것을  청윈 합니다  [SEP]',\n",
              " '[CLS] 다자녀의기준이 뭘까요?  오늘 상수도사업본부 감면신청을 보고 , 당황스러웠습니다  18세이하의 자녀3명이상이 신청대상이라네요    저희는  23살 2명, 21세 1명, 20세 1명 즉 4명의 자녀를 둔 학부모입니다   지금 이 시대를 살아가는 대가족의 한가정의로 안타까운 심정입니다  지방자치단체에서 다자녀혜택은 물론이고 , 올해로 대학생자녀가 3명이 되지만, 한국장학재단의 다자녀혜택을 보지못하고 있습니다  무엇이든지 행정이 정한 규칙이겠지만,  지금 이 어려운 시대를 살아가는 부모로써 슬픈현실입니다    [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTNZEiLw1Zjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [0] *5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOxM-Z9Q1ZBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3518766d-ce86-4204-dbf9-86ce70469e55"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print(sentences[0])\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] 소년법 폐지해주세요  법 아래에서 보호받아야 할 아이들이 법으로 인해 보호받지 못하고 있습니다 오히려 법을 악용하는 사례만 늘어나고 그 강도는 높아지고 있습니다 소년법폐지를 부탁드립니다 [SEP]\n",
            "['[CLS]', '소', '##년', '##법', '폐', '##지', '##해', '##주', '##세', '##요', '법', '아', '##래', '##에서', '보', '##호', '##받', '##아', '##야', '할', '아', '##이', '##들이', '법', '##으로', '인해', '보', '##호', '##받', '##지', '못', '##하고', '있', '##습', '##니다', '오', '##히', '##려', '법', '##을', '악', '##용', '##하는', '사', '##례', '##만', '늘', '##어', '##나', '##고', '그', '강', '##도는', '높', '##아', '##지고', '있', '##습', '##니다', '소', '##년', '##법', '##폐', '##지를', '부', '##탁', '##드', '##립', '##니다', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZZKwERG1YL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "1c62d41a-de4c-4812-e6a2-b1ceb8c45e31"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   9448,  10954,  33768,   9927,  12508,  14523,  16323,\n",
              "        24982,  48549,   9341,   9519,  37388,  11489,   9356,  20309,\n",
              "       118965,  16985,  21711,   9955,   9519,  10739,  20173,   9341,\n",
              "        11467,  39629,   9356,  20309, 118965,  12508,   9290,  12453,\n",
              "         9647, 119081,  48345,   9580,  18108,  26737,   9341,  10622,\n",
              "         9520,  24974,  12178,   9405,  58762,  19105,   9044,  12965,\n",
              "        16439,  11664,   8924,   8853,  60884,   9028,  16985,  68833,\n",
              "         9647, 119081,  48345,   9448,  10954,  33768, 119399,  36908,\n",
              "         9365, 119335,  15001,  35115,  48345,    102,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7eQZtOy1vug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eb3d23c3-422c-4b32-876b-fcb366b12852"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3KOB6fg1vnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a53b8240-b0e0-475b-c731-b4e02cbfe1e7"
      },
      "source": [
        "# 데이터를 파이토치의 텐서로 변환\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "print(test_inputs[0])\n",
        "print(test_labels[0])\n",
        "print(test_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   9448,  10954,  33768,   9927,  12508,  14523,  16323,  24982,\n",
            "         48549,   9341,   9519,  37388,  11489,   9356,  20309, 118965,  16985,\n",
            "         21711,   9955,   9519,  10739,  20173,   9341,  11467,  39629,   9356,\n",
            "         20309, 118965,  12508,   9290,  12453,   9647, 119081,  48345,   9580,\n",
            "         18108,  26737,   9341,  10622,   9520,  24974,  12178,   9405,  58762,\n",
            "         19105,   9044,  12965,  16439,  11664,   8924,   8853,  60884,   9028,\n",
            "         16985,  68833,   9647, 119081,  48345,   9448,  10954,  33768, 119399,\n",
            "         36908,   9365, 119335,  15001,  35115,  48345,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YCZrSmRxNBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 배치 사이즈\n",
        "#batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "#test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "#test_sampler = RandomSampler(test_data)\n",
        "#test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fYUM13r2ZCf",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "#**Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqNWVq21xR6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80bd9f38-85a2-463d-d5f9-105851f6cff2"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iduI9s_zxdhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 에폭수\n",
        "epochs = 10\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 학습률을 조금씩 감소시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3ggL2_O3A31",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Model Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4ewmEGxvND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvDYGlRw3Io4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUp1g0Y23IiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26c285ac-6323-4800-8636-37c416ea297f"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# 그래디언트 초기화\n",
        "model.zero_grad()\n",
        "\n",
        "# 에폭만큼 반복\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.3f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:43.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:35.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:06:43\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.858\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:53.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:45.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:37.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:06:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.852\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:45.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:38.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:06:46\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.852\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:44.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:36.\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:06:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.854\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:44.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:36.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:06:43\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.855\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:51.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:43.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:34.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:06:41\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.852\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:43.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:35.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:06:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.853\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:52.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:43.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:34.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:06:41\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.853\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:51.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:42.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:34.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:06:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.853\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch   500  of  1,800.    Elapsed: 0:01:51.\n",
            "  Batch 1,000  of  1,800.    Elapsed: 0:03:42.\n",
            "  Batch 1,500  of  1,800.    Elapsed: 0:05:33.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:06:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.853\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmtYsu5U3QeM",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **테스트셋 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP80FJx73IUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 입력 데이터 변환\n",
        "def convert_input_data(sentences):\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prrjvbP63IOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 테스트\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbAQhKbY3IHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34ffa168-4754-423f-feb6-cbd1795bc94c"
      },
      "source": [
        "logits = test_sentences([sentences[10]])\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3317Cl8bpzsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b846d36-272c-450a-8645-6e060c4f95bb"
      },
      "source": [
        "logits = test_sentences([sentences[9]])\n",
        "\n",
        "print(logits)\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.0210195 -2.9451287  5.7414165]]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9-moZdTwur1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0ed878e-5c6d-422e-8592-7db2a4a69340"
      },
      "source": [
        "%time\n",
        "category = [None]\n",
        "for i in range(len(sentences)):\n",
        "  logits = test_sentences([sentences[i]])\n",
        "  category.append(np.argmax(logits))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
            "Wall time: 5.25 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1sDpDrLD-VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category = category[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8Xguh2cbI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "4d399277-215c-49e3-95c8-9194cefe8931"
      },
      "source": [
        "%matplotlib inline \n",
        "plt.hist(category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1741.,    0.,    0.,    0.,    0., 1659.,    0.,    0.,    0.,\n",
              "        1600.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qllESGxxqvOB",
        "colab_type": "text"
      },
      "source": [
        "#**후처리 작업**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbXkYe4w_tsJ",
        "colab_type": "text"
      },
      "source": [
        "- pre-truncate: not good\n",
        "- reduce batch size: slightly better\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nEGTtCXbzJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_sent = train['data'][5000:6000]\n",
        "sample_sent = np.array(sample_sent)\n",
        "sample_lab = train['category'][5000:6000]\n",
        "sample_lab = np.array(sample_lab)\n",
        "sample_sent = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sample_sent]\n",
        "sample_sent = [sentence.replace('\\\\n', ' ') for sentence in sample_sent]\n",
        "sample_sent = [sentence.replace('.', ' ') for sentence in sample_sent]\n",
        "sample_sent = [sentence.replace('?', ' ') for sentence in sample_sent]\n",
        "sample_sent = [sentence.replace('!', ' ') for sentence in sample_sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AejZfChQlCbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "229ddd14-4771-4393-dcd1-d28c92cc463c"
      },
      "source": [
        "%time\n",
        "sample_category = [None]\n",
        "for i in range(len(sample_sent)):\n",
        "  temp = test_sentences([sample_sent[i]])\n",
        "  sample_category.append(np.argmax(temp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 7.39 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTcfOwsddfwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_category = sample_category[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD1AuqoshRo4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "12d72699-90b4-4996-e2b0-3b55fe9e8fb2"
      },
      "source": [
        "%matplotlib inline \n",
        "plt.hist(sample_category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([325.,   0.,   0.,   0.,   0., 324.,   0.,   0.,   0., 351.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hoh0pbOOhVAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "f7517754-6304-476e-ed36-cc94e4fab09a"
      },
      "source": [
        "temp = 0\n",
        "for i in range(len(sample_category)):\n",
        "    if sample_category[i] != sample_lab[i]: \n",
        "      temp = temp + 1\n",
        "      print('Row: {}, Actual Category: {}, Predicted Category: {}, Number of different rows: {}'.format(i, sample_lab[i], sample_category[i], temp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row: 6, Actual Category: 0, Predicted Category: 2, Number of different rows: 1\n",
            "Row: 28, Actual Category: 1, Predicted Category: 0, Number of different rows: 2\n",
            "Row: 29, Actual Category: 2, Predicted Category: 0, Number of different rows: 3\n",
            "Row: 71, Actual Category: 2, Predicted Category: 0, Number of different rows: 4\n",
            "Row: 82, Actual Category: 0, Predicted Category: 2, Number of different rows: 5\n",
            "Row: 133, Actual Category: 2, Predicted Category: 0, Number of different rows: 6\n",
            "Row: 157, Actual Category: 1, Predicted Category: 0, Number of different rows: 7\n",
            "Row: 177, Actual Category: 0, Predicted Category: 1, Number of different rows: 8\n",
            "Row: 178, Actual Category: 2, Predicted Category: 0, Number of different rows: 9\n",
            "Row: 181, Actual Category: 2, Predicted Category: 0, Number of different rows: 10\n",
            "Row: 187, Actual Category: 2, Predicted Category: 0, Number of different rows: 11\n",
            "Row: 195, Actual Category: 0, Predicted Category: 1, Number of different rows: 12\n",
            "Row: 205, Actual Category: 1, Predicted Category: 0, Number of different rows: 13\n",
            "Row: 283, Actual Category: 2, Predicted Category: 0, Number of different rows: 14\n",
            "Row: 290, Actual Category: 2, Predicted Category: 0, Number of different rows: 15\n",
            "Row: 294, Actual Category: 1, Predicted Category: 0, Number of different rows: 16\n",
            "Row: 300, Actual Category: 2, Predicted Category: 0, Number of different rows: 17\n",
            "Row: 324, Actual Category: 0, Predicted Category: 2, Number of different rows: 18\n",
            "Row: 345, Actual Category: 1, Predicted Category: 0, Number of different rows: 19\n",
            "Row: 362, Actual Category: 2, Predicted Category: 0, Number of different rows: 20\n",
            "Row: 406, Actual Category: 1, Predicted Category: 0, Number of different rows: 21\n",
            "Row: 410, Actual Category: 0, Predicted Category: 2, Number of different rows: 22\n",
            "Row: 478, Actual Category: 0, Predicted Category: 2, Number of different rows: 23\n",
            "Row: 536, Actual Category: 1, Predicted Category: 2, Number of different rows: 24\n",
            "Row: 537, Actual Category: 1, Predicted Category: 0, Number of different rows: 25\n",
            "Row: 610, Actual Category: 0, Predicted Category: 2, Number of different rows: 26\n",
            "Row: 617, Actual Category: 0, Predicted Category: 2, Number of different rows: 27\n",
            "Row: 640, Actual Category: 2, Predicted Category: 0, Number of different rows: 28\n",
            "Row: 672, Actual Category: 0, Predicted Category: 1, Number of different rows: 29\n",
            "Row: 718, Actual Category: 2, Predicted Category: 0, Number of different rows: 30\n",
            "Row: 724, Actual Category: 1, Predicted Category: 0, Number of different rows: 31\n",
            "Row: 740, Actual Category: 2, Predicted Category: 1, Number of different rows: 32\n",
            "Row: 761, Actual Category: 1, Predicted Category: 0, Number of different rows: 33\n",
            "Row: 778, Actual Category: 2, Predicted Category: 0, Number of different rows: 34\n",
            "Row: 817, Actual Category: 1, Predicted Category: 2, Number of different rows: 35\n",
            "Row: 829, Actual Category: 0, Predicted Category: 1, Number of different rows: 36\n",
            "Row: 848, Actual Category: 0, Predicted Category: 1, Number of different rows: 37\n",
            "Row: 856, Actual Category: 2, Predicted Category: 0, Number of different rows: 38\n",
            "Row: 870, Actual Category: 0, Predicted Category: 2, Number of different rows: 39\n",
            "Row: 892, Actual Category: 0, Predicted Category: 1, Number of different rows: 40\n",
            "Row: 921, Actual Category: 0, Predicted Category: 2, Number of different rows: 41\n",
            "Row: 981, Actual Category: 2, Predicted Category: 0, Number of different rows: 42\n",
            "Row: 984, Actual Category: 0, Predicted Category: 2, Number of different rows: 43\n",
            "Row: 996, Actual Category: 1, Predicted Category: 2, Number of different rows: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXqk1y5a49Cv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ad219cd6-2a97-4b42-fa06-3adda34a38e3"
      },
      "source": [
        "confusion_matrix(sample_lab, sample_category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[301,   6,  10],\n",
              "       [  9, 317,   3],\n",
              "       [ 15,   1, 338]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY43Ri4Nndyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb09ff90-ca17-49d6-c4ef-d0d65917ed1b"
      },
      "source": [
        "test_sentences([sample_sent[4]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.1471677, -4.3579984,  5.4787016]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMoO8gk5lr7w",
        "colab_type": "text"
      },
      "source": [
        "#**Out File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-7hZDGFEFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hTIkCrMMv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "718459e8-8257-449b-e4cc-af5d921da1f6"
      },
      "source": [
        "output = pd.DataFrame(category, columns=['category'])\n",
        "output['index'] = output.index\n",
        "output = output.iloc[:,[1,0]]\n",
        "print(output.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   index  category\n",
            "0      0         2\n",
            "1      1         2\n",
            "2      2         1\n",
            "3      3         0\n",
            "4      4         2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPCln_CKNfyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output.to_csv('data.csv')\n",
        "!cp data.csv \"drive/My Drive/NLP_practice/Blue-House/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ZCSEG0yPaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}